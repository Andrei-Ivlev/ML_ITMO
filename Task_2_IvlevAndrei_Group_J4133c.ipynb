{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd068d2d",
   "metadata": {},
   "source": [
    "# Lab 2. NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c03f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk import pos_tag\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039d69d",
   "metadata": {},
   "source": [
    "# Part I & II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b79709",
   "metadata": {},
   "source": [
    "1. Download Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt\n",
    "2. Perform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03fe10b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreiivlev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andreiivlev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andreiivlev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/andreiivlev/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(file_path):\n",
    "    # Load the stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Initialize the WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "\n",
    "    # Tokenize the text\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    # Perform POS tagging\n",
    "    pos_tagged = nltk.pos_tag(word_tokens)\n",
    "\n",
    "    # Lemmatize words with POS tags\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) if get_wordnet_pos(pos_tag) else word\n",
    "        for word, pos_tag in pos_tagged\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Join the words back into one string separated by space\n",
    "    preprocessed_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "file_path = '/Users/andreiivlev/Desktop/ITMO/MLT/task2/Alice.txt'\n",
    "\n",
    "preprocessed_text = preprocess_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2be3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5df0a8",
   "metadata": {},
   "source": [
    "# Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42e283",
   "metadata": {},
   "source": [
    "3. Find Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d2f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text.count('chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1cca4b",
   "metadata": {},
   "source": [
    "Word 'chapter' occurrs 24 times: 12 times for the table of contents and 12 times at the beginning of each chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab051b1b",
   "metadata": {},
   "source": [
    "For the chapter analysis we also need to drop everything after the end of the last chapter\n",
    "\n",
    "The end of the last chapter can be found by locating two consecutive 'end' words: 'end end' in the preprocessed text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ee33e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text.count('end end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adabcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text_ch = preprocessed_text.split('end end')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bedd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to chapters\n",
    "chapters = preprocessed_text_ch.split('chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c16a8",
   "metadata": {},
   "source": [
    "Now delete first 13 elements corresponding to the table of contents and everything prior to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590c67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = chapters[13:]\n",
    "#chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c110ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1:\n",
      "  say: 0.17505\n",
      "  eat: 0.17460\n",
      "  think: 0.16411\n",
      "  go: 0.16411\n",
      "  little: 0.16411\n",
      "  bat: 0.16190\n",
      "  get: 0.15317\n",
      "  door: 0.14627\n",
      "  key: 0.14302\n",
      "  see: 0.14223\n",
      "\n",
      "\n",
      "Chapter 2:\n",
      "  mouse: 0.30017\n",
      "  go: 0.22219\n",
      "  say: 0.20102\n",
      "  pool: 0.18441\n",
      "  little: 0.17986\n",
      "  cat: 0.15009\n",
      "  cry: 0.14145\n",
      "  dear: 0.13356\n",
      "  fan: 0.13047\n",
      "  foot: 0.12348\n",
      "\n",
      "\n",
      "Chapter 3:\n",
      "  say: 0.41651\n",
      "  mouse: 0.38774\n",
      "  dodo: 0.30817\n",
      "  lory: 0.15409\n",
      "  prize: 0.14952\n",
      "  dry: 0.14253\n",
      "  know: 0.13536\n",
      "  thimble: 0.11961\n",
      "  bird: 0.11078\n",
      "  cause: 0.10272\n",
      "\n",
      "\n",
      "Chapter 4:\n",
      "  bill: 0.20510\n",
      "  little: 0.20102\n",
      "  window: 0.20080\n",
      "  rabbit: 0.18177\n",
      "  puppy: 0.17570\n",
      "  say: 0.16606\n",
      "  grow: 0.15540\n",
      "  fan: 0.15089\n",
      "  go: 0.14858\n",
      "  get: 0.13984\n",
      "\n",
      "\n",
      "Chapter 5:\n",
      "  say: 0.45774\n",
      "  caterpillar: 0.45489\n",
      "  serpent: 0.27675\n",
      "  pigeon: 0.27675\n",
      "  egg: 0.13837\n",
      "  youth: 0.13837\n",
      "  size: 0.10993\n",
      "  father: 0.09903\n",
      "  think: 0.09637\n",
      "  little: 0.08834\n",
      "\n",
      "\n",
      "Chapter 6:\n",
      "  say: 0.39907\n",
      "  cat: 0.32044\n",
      "  footman: 0.25949\n",
      "  baby: 0.20428\n",
      "  mad: 0.18045\n",
      "  go: 0.17318\n",
      "  pig: 0.16714\n",
      "  duchess: 0.15660\n",
      "  grin: 0.14764\n",
      "  wow: 0.12974\n",
      "\n",
      "\n",
      "Chapter 7:\n",
      "  hatter: 0.45201\n",
      "  say: 0.43381\n",
      "  dormouse: 0.41866\n",
      "  hare: 0.25818\n",
      "  march: 0.23411\n",
      "  go: 0.12574\n",
      "  twinkle: 0.12405\n",
      "  time: 0.10688\n",
      "  tea: 0.09588\n",
      "  well: 0.09431\n",
      "\n",
      "\n",
      "Chapter 8:\n",
      "  queen: 0.43955\n",
      "  say: 0.34487\n",
      "  hedgehog: 0.21072\n",
      "  king: 0.20088\n",
      "  go: 0.17610\n",
      "  gardener: 0.16858\n",
      "  look: 0.15409\n",
      "  soldier: 0.14387\n",
      "  cat: 0.14312\n",
      "  five: 0.12668\n",
      "\n",
      "\n",
      "Chapter 9:\n",
      "  say: 0.46483\n",
      "  turtle: 0.40210\n",
      "  mock: 0.38721\n",
      "  gryphon: 0.26735\n",
      "  duchess: 0.19294\n",
      "  moral: 0.17668\n",
      "  queen: 0.15494\n",
      "  go: 0.13671\n",
      "  think: 0.08203\n",
      "  day: 0.06903\n",
      "\n",
      "\n",
      "Chapter 10:\n",
      "  turtle: 0.41138\n",
      "  mock: 0.37157\n",
      "  gryphon: 0.36925\n",
      "  say: 0.30455\n",
      "  dance: 0.26238\n",
      "  lobster: 0.24489\n",
      "  soup: 0.15924\n",
      "  beautiful: 0.15924\n",
      "  soooop: 0.12245\n",
      "  join: 0.10720\n",
      "\n",
      "\n",
      "Chapter 11:\n",
      "  king: 0.39887\n",
      "  hatter: 0.35893\n",
      "  say: 0.32950\n",
      "  court: 0.29023\n",
      "  dormouse: 0.25153\n",
      "  witness: 0.22530\n",
      "  queen: 0.11431\n",
      "  officer: 0.11265\n",
      "  juror: 0.11265\n",
      "  begin: 0.10199\n",
      "\n",
      "\n",
      "Chapter 12:\n",
      "  say: 0.45672\n",
      "  king: 0.38527\n",
      "  jury: 0.19510\n",
      "  dream: 0.17669\n",
      "  write: 0.15879\n",
      "  queen: 0.14499\n",
      "  sister: 0.13657\n",
      "  would: 0.11642\n",
      "  slate: 0.11043\n",
      "  rabbit: 0.10642\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exclude_words = ['alice', 'im']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=exclude_words)\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(chapters)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "top_words_per_chapter = {}\n",
    "\n",
    "for chapter_idx in range(tfidf_matrix.shape[0]):\n",
    "    row = tfidf_matrix.getrow(chapter_idx)\n",
    "    \n",
    "    scores = row.toarray().flatten()\n",
    "    sorted_indices = scores.argsort()[-10:][::-1]  # Indices of the top 10 scores\n",
    "    \n",
    "    top_words_per_chapter[chapter_idx+1] = [(feature_names[idx], scores[idx]) for idx in sorted_indices]\n",
    "\n",
    "for chapter, words in top_words_per_chapter.items():\n",
    "    print(f\"Chapter {chapter}:\")\n",
    "    for word, score in words:\n",
    "        print(f\"  {word}: {score:.5f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9b68b",
   "metadata": {},
   "source": [
    "**Chapter's names**\n",
    "\n",
    "1. {say, eat, think, go, little, bat, get, door, key, see} -- 'Bat shows the way to the little door'\n",
    "2. {mouse, go, say, pool, little, cat, cry, deer, fan, foot} -- 'Cat chases little mouse by the pool'\n",
    "3. {say, mouse, dodo, lory, prize, dry, know, thimble, bird, cause} -- 'Lory plays thimbles to win a prize from dodo'\n",
    "4. {bill, little, window, rabbit, puppy, say, grow, fan, go, get} -- 'Bill and little rabbit's adventure'\n",
    "5. {say, caterpillar, serpent, pigeon, egg, youth, size, father, think, little} -- 'Pigeon fights caterpillar to protect an egg'\n",
    "6. {say, cat, footman, baby, mad, go, pig, duchess, grin, wow} -- 'Duchess, footman and the baby cook the big'\n",
    "7. {hatter, say, doormouse, hare, march, go, twinkle, time, tea, well} -- 'Tea-party with the Hatter and Dormouse'\n",
    "8. {queen, say, hedgehog, king, go, gardener, look, soldier, cat, five} -- 'Royal Tangles in the Garden'\n",
    "9. {say, turtle, mock, gryphon, duchess, moral, queen, go, think, day} -- 'Tales of the Mock Turtle'\n",
    "10. {turtle, mock, gryphon, say, dance, lobster, soup, beautiful, join} -- 'Beatiful lobster soup from the Mock Turtle'\n",
    "11. {king, hatter, say, court, dormouse, witness, queen, officer, juror, begin} -- 'Hatter's trial: Royal Family'\n",
    "12. {say, king, jury, dream, write, queen, sister, would, slate, rabbit} -- 'Vivid dream: King vs Queen and her sister'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbf888",
   "metadata": {},
   "source": [
    "# Part IV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecdf75d",
   "metadata": {},
   "source": [
    "Find the Top 10 most used verbs in sentences with Alice. What does Alice do most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a24e3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text_verbs = preprocessed_text_ch.split('chapter xii')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e5bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 verbs used in the vicinity of 'Alice':\n",
      "say: 226\n",
      "go: 50\n",
      "think: 43\n",
      "get: 26\n",
      "come: 26\n",
      "know: 25\n",
      "see: 20\n",
      "begin: 19\n",
      "take: 13\n",
      "make: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/andreiivlev/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "words = preprocessed_text_verbs.split()\n",
    "\n",
    "alice_verbs = []\n",
    "\n",
    "window_size = 5 \n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    if word == 'alice':\n",
    "        # Define the context window\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, len(words))\n",
    "        context = words[start:end]\n",
    "\n",
    "        pos_tags = pos_tag(context)\n",
    "        for context_word, tag in pos_tags:\n",
    "            if tag.startswith('V') and context_word != 'alice':\n",
    "                alice_verbs.append(context_word)\n",
    "\n",
    "verb_freq = Counter(alice_verbs)\n",
    "\n",
    "# Get the top 10 most frequent verbs\n",
    "top_verbs = verb_freq.most_common(10)\n",
    "\n",
    "print(\"Top 10 verbs used in the vicinity of 'Alice':\")\n",
    "for verb, freq in top_verbs:\n",
    "    print(f\"{verb}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ea4c9",
   "metadata": {},
   "source": [
    "Alice **says** a lot of things, **goes** places, **thinks** a lot. She **gets** what she wants, **comes** to several locations. Alice **knows** and **sees** things, she also **begins**, **takes** and **makes** somehting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
